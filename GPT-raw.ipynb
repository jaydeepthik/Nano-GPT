{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data file if not done already\n",
    "#! curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt --output ./data/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of data : 1115394\n"
     ]
    }
   ],
   "source": [
    "#read dataset\n",
    "with open(\"./data/input.txt\", 'r', encoding='utf-8') as f:\n",
    "  all_data =  f.read()\n",
    "\n",
    "print(f\"Total length of data : {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2, 2]\n",
      "Hello world!!\n"
     ]
    }
   ],
   "source": [
    "#Build vocab\n",
    "unique_chars = sorted(list(set(all_data)))\n",
    "vocab_size = len(unique_chars)\n",
    "\n",
    "#create mappings between chars and index\n",
    "ctoi = {ch:id_ for id_, ch in enumerate(unique_chars)}\n",
    "itoc = {id_:ch for id_, ch in enumerate(unique_chars)}\n",
    "\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda ids: \"\".join([itoc[i] for i in ids])\n",
    "\n",
    "print(encode(\"Hello world!!\"))\n",
    "print(decode(encode(\"Hello world!!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "#Encode the dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(all_data), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validataion datasets\n",
    "split_thresh = int(0.9*len(data)) #first 90% as train, remaining 10% as validation\n",
    "train_data = data[:split_thresh]\n",
    "val_data = data[split_thresh:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs : torch.Size([4, 8])\n",
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
      "outputs : torch.Size([4, 8])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
      "##########################################\n",
      "When context is tensor([57]), target is 1\n",
      "When context is tensor([57,  1]), target is 46\n",
      "When context is tensor([57,  1, 46]), target is 47\n",
      "When context is tensor([57,  1, 46, 47]), target is 57\n",
      "When context is tensor([57,  1, 46, 47, 57]), target is 1\n",
      "When context is tensor([57,  1, 46, 47, 57,  1]), target is 50\n",
      "When context is tensor([57,  1, 46, 47, 57,  1, 50]), target is 53\n",
      "When context is tensor([57,  1, 46, 47, 57,  1, 50, 53]), target is 60\n",
      "When context is tensor([1]), target is 58\n",
      "When context is tensor([ 1, 58]), target is 46\n",
      "When context is tensor([ 1, 58, 46]), target is 43\n",
      "When context is tensor([ 1, 58, 46, 43]), target is 56\n",
      "When context is tensor([ 1, 58, 46, 43, 56]), target is 43\n",
      "When context is tensor([ 1, 58, 46, 43, 56, 43]), target is 1\n",
      "When context is tensor([ 1, 58, 46, 43, 56, 43,  1]), target is 41\n",
      "When context is tensor([ 1, 58, 46, 43, 56, 43,  1, 41]), target is 39\n",
      "When context is tensor([17]), target is 26\n",
      "When context is tensor([17, 26]), target is 15\n",
      "When context is tensor([17, 26, 15]), target is 17\n",
      "When context is tensor([17, 26, 15, 17]), target is 10\n",
      "When context is tensor([17, 26, 15, 17, 10]), target is 0\n",
      "When context is tensor([17, 26, 15, 17, 10,  0]), target is 32\n",
      "When context is tensor([17, 26, 15, 17, 10,  0, 32]), target is 53\n",
      "When context is tensor([17, 26, 15, 17, 10,  0, 32, 53]), target is 1\n",
      "When context is tensor([57]), target is 58\n",
      "When context is tensor([57, 58]), target is 6\n",
      "When context is tensor([57, 58,  6]), target is 1\n",
      "When context is tensor([57, 58,  6,  1]), target is 61\n",
      "When context is tensor([57, 58,  6,  1, 61]), target is 47\n",
      "When context is tensor([57, 58,  6,  1, 61, 47]), target is 58\n",
      "When context is tensor([57, 58,  6,  1, 61, 47, 58]), target is 46\n",
      "When context is tensor([57, 58,  6,  1, 61, 47, 58, 46]), target is 0\n"
     ]
    }
   ],
   "source": [
    "max_context_length = 8\n",
    "batch_size = 4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split==\"train\" else val_data\n",
    "  rand_idx = torch.randint(len(data) - max_context_length, (batch_size, ))\n",
    "  X = torch.stack([data[i:i+max_context_length] for i in rand_idx])\n",
    "  y = torch.stack([data[i+1:i+max_context_length+1] for i in rand_idx])\n",
    "  return X, y\n",
    "\n",
    "#test and sanity check\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(f\"inputs : {xb.shape}\")\n",
    "print(xb)\n",
    "print(f\"outputs : {yb.shape}\")\n",
    "print(yb)\n",
    "\n",
    "print(\"##########################################\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(max_context_length):\n",
    "    context = xb[b,:t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f\"When context is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([32, 65])\n",
      "output loss: 4.886534690856934\n",
      "\n",
      "uoiaF$z\n",
      "M?kI;h\n",
      "DbuMG,H3LYNmrDxKgTpvAKOF-jU.hc;fBMTGa-IS\n",
      "g3lEb&ZQ,l;:m;lpcNN\n",
      "KpVEYRIIM,'hCRbMAcWTkrnH\n"
     ]
    }
   ],
   "source": [
    "#simplest language model: BigramLanguage model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "      super().__init__()\n",
    "      self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "  \n",
    "  def forward(self, idx, targets = None):\n",
    "      #shape idx: (B, T), shape targets: (B, T)\n",
    "      logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "      if targets is None:\n",
    "        loss = None\n",
    "      else:\n",
    "        # tensor conversion from (B, T, C) to (B, C, T) as per cross_entropy documentation\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C) #reshaped to (B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "      \n",
    "      return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_context_length):\n",
    "      #idx is of shape: (B, T)\n",
    "      for _ in range(max_context_length):\n",
    "        #get predictions\n",
    "        logits, loss = self(idx)\n",
    "        #use only the last time step\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        #sample from the distribution\n",
    "\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "      return idx\n",
    "\n",
    "\n",
    "bgm = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bgm(xb, yb)\n",
    "\n",
    "print(f\"output shape: {logits.shape}\")\n",
    "print(f\"output loss: {loss.item()}\")\n",
    "\n",
    "#test the generator\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(f\"{decode(bgm.generate(idx, max_context_length=100)[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch oprimization objective\n",
    "optimizer = torch.optim.AdamW(bgm.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.193429946899414\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "    \n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    #evaluate and train\n",
    "    logits, loss = bgm(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tX;f'czQfffUN mPxn'CUThWor bl;UJsDBEYheOrorrecdyJk-lmisf.xYMr cINusjzGE&,C w,SneceroGPHao in:EPey.wy y!IArg$XWRAnumWk \n",
      "MA .\n",
      "JSjVz,LS bor?YRDul.kneo fuzI-b I.OMm pteaveMAu HPck-K?\n",
      "QuYy, yZ'\n",
      "\n",
      "ON.hedsw,laeegee\n",
      "OPABludonach ylDWhve\n",
      "'d;iU.c&I;.t\n",
      "\n",
      "\n",
      "WCu:mak-KY?x3LD,abaqPMn,\n",
      "Jon&GTOhouUur-e?ac'wO:manUNUp-nt;h b&r'piQz;'KCB: agshhnilwat,\n",
      "&3TscGpwnttr&INLHXERLJPryNnegid,\n",
      "Jq pI njbaowhe FfaPF3AVB\n",
      "yZgh'dJlnt&\n"
     ]
    }
   ],
   "source": [
    "gen_idx =torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(bgm.generate(gen_idx, max_context_length=400)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## averaging past tokens: basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= :\n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b= :\n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c= :\n",
      " tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "#trick using matrix multiplication\n",
    "#version 1\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a/torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print(f'a= :\\n {a}')\n",
    "print(f'b= :\\n {b}')\n",
    "print(f'c= :\\n {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trick using matrix multiplication vectorised\n",
    "# version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei/wei.sum(1, keepdim=True)\n",
    "\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) ---> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 3 using softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = torch.masked_fill(wei, tril ==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 4 :single head of self attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "q = query(x) # (B, T, head_size)\n",
    "k = key(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 #(B, T, head_size) @ (B, head_size, T) ----> (B, T, T), scaled attention to preserve variance in the weights\n",
    "#tril = torch.tril(torch.ones((T, T)))\n",
    "\n",
    "#wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0449), tensor(1.0700), tensor(17.4690))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0449), tensor(1.0700), tensor(1.0918))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
